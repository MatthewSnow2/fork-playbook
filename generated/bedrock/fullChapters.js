// Generated by Adaptive Learning Curriculum Generator
// Topic: AWS Bedrock Fundamentals
// Generated: 2026-01-09T03:24:51.059Z

export const fullChapterContent = {
  "1": {
    "sections": [
      {
        "title": "What is AWS Bedrock",
        "content": "### What is AWS Bedrock\n\n**AWS Bedrock** is Amazon's fully managed service that provides secure access to foundation models (FMs) from leading AI companies through a unified API. As a serverless platform, Bedrock eliminates the need to manage infrastructure while offering enterprise-grade security and privacy controls.\n\n#### Serverless Architecture Benefits\n\nBedrock's serverless design means you pay only for what you use, with no upfront costs or infrastructure management overhead. The service automatically scales to handle your workload demands, from prototype development to production-scale applications.\n\nüéì **Key Features:**\n- **Model Diversity**: Access multiple AI providers through a single interface\n- **Serverless Operations**: No server provisioning or maintenance required\n- **Enterprise Security**: Built-in data encryption and privacy controls\n- **API Integration**: RESTful APIs and AWS SDK support\n\n#### Position in AWS AI/ML Portfolio\n\nBedrock complements AWS's comprehensive AI/ML ecosystem:\n\n| Service | Purpose | Use Case |\n|---------|---------|----------|\n| **SageMaker** | Custom model training/hosting | Build proprietary models |\n| **Bedrock** | Foundation model access | Leverage pre-trained models |\n| **Comprehend** | NLP services | Document analysis |\n| **Rekognition** | Computer vision | Image/video analysis |\n\nüí° **Practical Example**: A content marketing team can use Bedrock to generate blog posts with Claude 3, analyze sentiment with Cohere, and create summaries with Amazon Titan‚Äîall without managing any ML infrastructure.\n\n#### Integration Capabilities\n\nBedrock seamlessly integrates with AWS services like Lambda for serverless applications, API Gateway for web interfaces, and S3 for data storage. This integration enables rapid development of AI-powered applications using familiar AWS tools and patterns.\n\n‚ö†Ô∏è **Important**: Bedrock requires explicit model access requests for each foundation model before use, ensuring controlled and compliant AI adoption within organizations."
      },
      {
        "title": "Foundation Models Ecosystem",
        "content": "### Foundation Models Ecosystem\n\nAWS Bedrock hosts a diverse ecosystem of **foundation models** from industry-leading AI companies, each optimized for specific use cases and performance characteristics. This variety enables developers to choose the most suitable model for their application requirements.\n\n#### Available Model Providers\n\n**Anthropic Models**\n- **Claude 3 (Opus, Sonnet, Haiku)**: Advanced reasoning, analysis, and creative tasks\n- **Use Cases**: Complex document analysis, code generation, research assistance\n- **Strengths**: Constitutional AI training, safety-focused responses\n\n**AI21 Labs**\n- **Jurassic-2**: Multilingual text generation and comprehension\n- **Use Cases**: Content creation, translation, summarization\n- **Strengths**: Instruction-following, contextual understanding\n\n**Cohere**\n- **Command**: Text generation and conversation\n- **Embed**: Text embeddings for semantic search\n- **Use Cases**: Chatbots, search applications, content classification\n\n**Meta**\n- **Llama 2**: Open-source foundation model for various NLP tasks\n- **Use Cases**: Research, fine-tuning, specialized applications\n- **Strengths**: Transparency, customization potential\n\n**Amazon Titan**\n- **Titan Text**: Amazon's proprietary text generation model\n- **Titan Embeddings**: High-quality text embeddings\n- **Use Cases**: Search, personalization, content generation\n\nüíé **Model Selection Example**:\n```\nScenario: E-commerce product descriptions\n- Primary: Amazon Titan Text (cost-effective, retail-optimized)\n- Alternative: Claude 3 Haiku (creative, brand-focused content)\n- Embeddings: Titan Embeddings (product search, recommendations)\n```\n\nüîß **Best Practices**:\n- **Benchmark models** with your specific use cases\n- **Consider latency requirements** when choosing between model sizes\n- **Evaluate cost vs. performance** trade-offs for production workloads\n- **Test multilingual capabilities** if serving global audiences\n\nEach model offers unique capabilities, making it essential to understand their strengths and align them with your application's specific requirements for optimal results."
      },
      {
        "title": "Service Architecture and Security",
        "content": "### Service Architecture and Security\n\nAWS Bedrock's architecture prioritizes **security, scalability, and data privacy** while delivering high-performance AI capabilities. Understanding this architecture is crucial for implementing enterprise-grade AI solutions.\n\n#### Infrastructure Architecture\n\nBedrock operates on a **multi-tenant, serverless infrastructure** that automatically scales based on demand:\n\n- **API Gateway Layer**: Handles authentication, rate limiting, and request routing\n- **Model Serving Layer**: Manages model loading, inference, and response generation\n- **Security Layer**: Enforces access controls, encryption, and audit logging\n- **Monitoring Layer**: Tracks usage, performance, and operational metrics\n\nüéì **Key Architectural Principles**:\n- **Isolation**: Each customer's data and requests are isolated\n- **Scalability**: Automatic scaling without capacity planning\n- **Availability**: Multi-AZ deployment for high availability\n- **Performance**: Optimized model serving infrastructure\n\n#### Data Privacy and Security\n\n**Data Protection**:\n- **In-Transit Encryption**: All API calls use TLS 1.2+\n- **At-Rest Encryption**: Customer data encrypted with AWS KMS\n- **Data Residency**: No customer data used for model training\n- **Retention Policy**: Prompts and responses not stored by default\n\n**Access Controls**:\n```json\n{\n  \"Effect\": \"Allow\",\n  \"Action\": \"bedrock:InvokeModel\",\n  \"Resource\": \"arn:aws:bedrock:*:*:foundation-model/anthropic.claude-v2\",\n  \"Condition\": {\n    \"StringEquals\": {\n      \"aws:RequestedRegion\": \"us-east-1\"\n    }\n  }\n}\n```\n\nüí° **Security Features**:\n- **IAM Integration**: Fine-grained access control using AWS IAM\n- **VPC Support**: Private network access through VPC endpoints\n- **CloudTrail Logging**: Comprehensive API call auditing\n- **Guardrails**: Content filtering and safety controls\n\n‚ö†Ô∏è **Compliance Considerations**:\n- **SOC 2 Type 2** certified\n- **GDPR** compliant data handling\n- **HIPAA** eligible for healthcare workloads\n- **ISO 27001** security standards\n\n‚úÖ **Best Practices**:\n- Implement least-privilege IAM policies\n- Use VPC endpoints for sensitive workloads\n- Enable CloudTrail for audit compliance\n- Configure Guardrails for content safety"
      },
      {
        "title": "Getting Started with Console",
        "content": "### Getting Started with Console\n\nThe **AWS Bedrock Console** provides an intuitive interface for exploring foundation models, testing prompts, and managing your AI applications. This section guides you through the initial setup and console navigation.\n\n#### Initial Setup Requirements\n\n**Prerequisites**:\n- AWS account with appropriate permissions\n- IAM role with Bedrock access policies\n- Model access requests approved (required for each model)\n\nüîß **Step-by-Step Setup**:\n\n1. **Navigate to Bedrock Console**\n   - Go to AWS Console ‚Üí Search \"Bedrock\"\n   - Select your preferred region (us-east-1, us-west-2 recommended)\n\n2. **Request Model Access**\n   - Click \"Model access\" in left navigation\n   - Select desired models (Claude 3, Titan, etc.)\n   - Submit access request (approval typically within 24 hours)\n\n3. **Verify Permissions**\n   ```json\n   {\n     \"Version\": \"2012-10-17\",\n     \"Statement\": [{\n       \"Effect\": \"Allow\",\n       \"Action\": [\n         \"bedrock:InvokeModel\",\n         \"bedrock:ListFoundationModels\",\n         \"bedrock:GetFoundationModel\"\n       ],\n       \"Resource\": \"*\"\n     }]\n   }\n   ```\n\n#### Console Interface Navigation\n\n**Main Dashboard Sections**:\n- **Overview**: Service introduction and getting started guides\n- **Text/Image Playgrounds**: Interactive model testing environments\n- **Model Access**: Request and manage foundation model permissions\n- **Custom Models**: Fine-tuning and custom model management\n- **Guardrails**: Configure content filtering and safety controls\n\nüí° **Text Playground Example**:\n1. Select \"Text playground\" from navigation\n2. Choose model (e.g., \"Claude 3 Sonnet\")\n3. Configure parameters:\n   - **Temperature**: 0.7 (creativity level)\n   - **Maximum tokens**: 500\n   - **Top P**: 0.9 (response diversity)\n4. Enter prompt: \"Write a product description for a smart home thermostat\"\n5. Click \"Run\" to generate response\n\nüéì **Console Tips**:\n- **Save configurations** for reusable prompt templates\n- **Compare models** side-by-side for optimal selection\n- **Export code samples** for integration into applications\n- **Monitor usage** through built-in metrics dashboard\n\n‚úÖ **Next Steps**: After console familiarity, explore programmatic access using AWS SDKs and APIs for production applications."
      },
      {
        "title": "Pricing and Cost Optimization",
        "content": "### Pricing and Cost Optimization\n\nUnderstanding AWS Bedrock's **pricing models** is essential for optimizing costs while delivering high-performance AI applications. Bedrock offers flexible pricing options to match different usage patterns and budget requirements.\n\n#### Pricing Models Overview\n\n**On-Demand Pricing**\n- **Pay-per-use**: Charged per input/output token\n- **No commitments**: Ideal for variable or unpredictable workloads\n- **Immediate availability**: No provisioning delays\n\n**Provisioned Throughput**\n- **Reserved capacity**: Guaranteed model availability\n- **Hourly pricing**: Fixed costs for consistent workloads\n- **Performance benefits**: Lower latency, higher throughput\n\n#### Cost Comparison Example\n\n| Model | On-Demand (per 1K tokens) | Provisioned (per hour) |\n|-------|---------------------------|------------------------|\n| **Claude 3 Sonnet** | Input: $0.003, Output: $0.015 | $25.00 |\n| **Claude 3 Haiku** | Input: $0.00025, Output: $0.00125 | $3.00 |\n| **Titan Text** | Input: $0.0008, Output: $0.0016 | $12.00 |\n\nüí° **Break-Even Analysis**:\n```\nScenario: 10,000 requests/day, 100 tokens input, 200 tokens output\n\nClaude 3 Haiku On-Demand:\n(100 √ó $0.00025 + 200 √ó $0.00125) √ó 10,000 = $27.50/day\n\nProvisioned Throughput: $3.00/hour √ó 24 = $72/day\n\nResult: On-demand more cost-effective for this usage pattern\n```\n\n#### Cost Optimization Strategies\n\nüîß **Model Selection Optimization**:\n- **Use smaller models** (Haiku vs. Opus) for simpler tasks\n- **Implement model routing** based on complexity requirements\n- **Cache responses** for repeated queries\n- **Optimize prompt length** to reduce input token costs\n\n**Usage Pattern Analysis**:\n- **Monitor token consumption** using CloudWatch metrics\n- **Identify peak usage periods** for provisioned throughput decisions\n- **Implement request batching** to reduce API overhead\n\n‚ö†Ô∏è **Cost Monitoring**:\n- Set up **AWS Budgets** alerts for spending thresholds\n- Use **Cost Explorer** to analyze usage trends\n- Implement **tagging strategies** for cost attribution\n- Configure **guardrails** to prevent unexpected usage spikes\n\nüíé **Best Practices**:\n- **Start with on-demand** for prototyping and testing\n- **Switch to provisioned** when usage patterns stabilize\n- **Regular cost reviews** to optimize model selection\n- **Implement caching layers** to reduce redundant API calls\n\nEffective cost management requires ongoing monitoring and optimization based on actual usage patterns and business requirements."
      }
    ]
  },
  "2": {
    "sections": [
      {
        "title": "Bedrock API Fundamentals",
        "content": "### Bedrock API Fundamentals\n\nAWS Bedrock provides REST API endpoints that enable seamless integration with foundation models through standardized HTTP requests. Understanding the core API structure is essential for building robust text generation applications.\n\n#### üîß API Endpoint Structure\n\nBedrock uses model-specific endpoints following this pattern:\n```\nhttps://bedrock-runtime.{region}.amazonaws.com/model/{model-id}/invoke\n```\n\nFor example:\n- Claude 3: `anthropic.claude-3-sonnet-20240229-v1:0`\n- Llama 2: `meta.llama2-70b-chat-v1`\n\n#### üéì Authentication Setup\n\nBedrock requires AWS IAM authentication with the `bedrock:InvokeModel` permission:\n\n```python\n# Python SDK Configuration\nimport boto3\nfrom botocore.config import Config\n\n# Initialize client with retry configuration\nbedrock = boto3.client(\n    'bedrock-runtime',\n    region_name='us-east-1',\n    config=Config(\n        retries={'max_attempts': 3, 'mode': 'adaptive'},\n        read_timeout=300\n    )\n)\n```\n\n```javascript\n// Node.js SDK Configuration\nconst { BedrockRuntimeClient } = require('@aws-sdk/client-bedrock-runtime');\n\nconst client = new BedrockRuntimeClient({\n    region: 'us-east-1',\n    maxAttempts: 3,\n    requestTimeout: 300000\n});\n```\n\n#### üí° SDK Best Practices\n\n**Connection Pooling**: Reuse client instances across requests to avoid connection overhead.\n\n**Region Selection**: Choose regions closest to your users for optimal latency.\n\n**Timeout Configuration**: Set appropriate timeouts (30-300 seconds) based on your model and prompt complexity.\n\n#### ‚ö†Ô∏è Common Setup Issues\n\n- **Permission Denied**: Ensure your IAM role has `bedrock:InvokeModel` permissions\n- **Region Mismatch**: Verify model availability in your selected region\n- **Quota Limits**: Check service quotas for your account\n\nThe foundation is now set for implementing specific model integrations with proper authentication and error handling."
      },
      {
        "title": "Text Generation with Claude",
        "content": "### Text Generation with Claude\n\nClaude models from Anthropic offer powerful conversational AI capabilities through Bedrock. This section demonstrates implementing text generation with proper prompt structuring and response handling.\n\n#### üéì Basic Claude Integration\n\n```python\nimport json\n\ndef generate_with_claude(prompt, max_tokens=1000):\n    body = json.dumps({\n        \"anthropic_version\": \"bedrock-2023-05-31\",\n        \"max_tokens\": max_tokens,\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": prompt\n            }\n        ],\n        \"temperature\": 0.7,\n        \"top_p\": 0.9\n    })\n    \n    response = bedrock.invoke_model(\n        modelId=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n        body=body\n    )\n    \n    response_body = json.loads(response['body'].read())\n    return response_body['content'][0]['text']\n```\n\n#### üí° Advanced Prompt Structuring\n\nClaude responds well to structured conversations with clear role definitions:\n\n```python\ndef structured_claude_prompt(system_prompt, user_message):\n    body = json.dumps({\n        \"anthropic_version\": \"bedrock-2023-05-31\",\n        \"max_tokens\": 1000,\n        \"system\": system_prompt,\n        \"messages\": [\n            {\n                \"role\": \"user\",\n                \"content\": user_message\n            }\n        ]\n    })\n    \n    # Process response...\n```\n\n**Example Implementation**:\n```python\nsystem_prompt = \"You are a technical writing assistant specializing in API documentation.\"\nuser_message = \"Explain how to implement rate limiting in REST APIs.\"\n\nresult = structured_claude_prompt(system_prompt, user_message)\n```\n\n#### üîß Model-Specific Features\n\n**Multi-turn Conversations**: Claude maintains context across message exchanges:\n\n```python\nmessages = [\n    {\"role\": \"user\", \"content\": \"What's the weather like?\"},\n    {\"role\": \"assistant\", \"content\": \"I don't have access to current weather data.\"},\n    {\"role\": \"user\", \"content\": \"How would I get weather data programmatically?\"}\n]\n```\n\n#### ‚úÖ Response Quality Tips\n\n- **Be Specific**: Detailed prompts yield better responses\n- **Set Context**: Use system prompts to establish expertise domains\n- **Iterate**: Refine prompts based on output quality"
      },
      {
        "title": "Prompt Engineering Best Practices",
        "content": "### Prompt Engineering Best Practices\n\nEffective prompt engineering is crucial for consistent, high-quality outputs from foundation models. This section covers advanced techniques that significantly improve response quality and reliability.\n\n#### üéì Few-Shot Learning Technique\n\nFew-shot prompting provides examples to guide model behavior:\n\n```python\ndef few_shot_prompt_example():\n    prompt = \"\"\"\n    Extract key information from customer reviews:\n    \n    Example 1:\n    Review: \"Great product, fast shipping, but packaging was damaged.\"\n    Output: {\"sentiment\": \"mixed\", \"shipping\": \"positive\", \"packaging\": \"negative\"}\n    \n    Example 2:\n    Review: \"Excellent quality, highly recommend!\"\n    Output: {\"sentiment\": \"positive\", \"quality\": \"positive\", \"recommendation\": \"positive\"}\n    \n    Now extract from this review:\n    Review: \"Product works well but customer service was unhelpful.\"\n    Output:\n    \"\"\"\n    \n    return generate_with_claude(prompt, max_tokens=200)\n```\n\n#### üíé Role-Based Prompting\n\nAssign specific roles to improve response quality:\n\n```python\nrole_prompts = {\n    \"technical_writer\": \"You are a senior technical writer with 10 years of experience in API documentation.\",\n    \"data_analyst\": \"You are a data analyst specializing in business intelligence and statistical analysis.\",\n    \"code_reviewer\": \"You are an experienced software architect reviewing code for best practices.\"\n}\n\ndef role_based_generation(role, task):\n    system_prompt = role_prompts[role]\n    return structured_claude_prompt(system_prompt, task)\n```\n\n#### üîß Chain-of-Thought Prompting\n\nEncourage step-by-step reasoning:\n\n```python\ncot_prompt = \"\"\"\nSolve this problem step by step:\n\nProblem: Calculate the monthly AWS costs for a web application with:\n- 1 million API calls per month\n- 500GB data transfer\n- 2 EC2 t3.medium instances running 24/7\n\nThink through each cost component:\n1. First, identify all billable services\n2. Then, calculate each cost component\n3. Finally, sum the total monthly cost\n\"\"\"\n```\n\n#### ‚ö†Ô∏è Common Pitfalls to Avoid\n\n| Issue | Problem | Solution |\n|-------|---------|----------|\n| **Ambiguous Instructions** | Vague requirements | Use specific, measurable criteria |\n| **Context Overload** | Too much information | Focus on essential context only |\n| **Inconsistent Examples** | Conflicting patterns | Ensure examples follow same format |\n\n#### üí° Optimization Strategies\n\n- **Template Reuse**: Create reusable prompt templates for common tasks\n- **A/B Testing**: Compare different prompt variations\n- **Progressive Refinement**: Iteratively improve prompts based on outputs"
      },
      {
        "title": "Model Parameters and Configuration",
        "content": "### Model Parameters and Configuration\n\nFine-tuning model parameters is essential for optimizing text generation quality and behavior. Understanding how temperature, top-k, and top-p affect outputs enables precise control over model creativity and consistency.\n\n#### üéì Core Parameters Explained\n\n**Temperature** (0.0 - 1.0): Controls randomness in token selection\n- `0.0`: Deterministic, always selects highest probability token\n- `0.7`: Balanced creativity and coherence\n- `1.0`: Maximum randomness\n\n**Top-p** (0.0 - 1.0): Nucleus sampling - considers tokens up to cumulative probability\n- `0.1`: Very focused, limited vocabulary\n- `0.9`: Diverse but coherent\n- `1.0`: All tokens considered\n\n**Top-k** (1-500): Limits consideration to top-k highest probability tokens\n\n#### üîß Parameter Configuration Examples\n\n```python\n# Creative writing configuration\ncreative_config = {\n    \"temperature\": 0.8,\n    \"top_p\": 0.9,\n    \"top_k\": 200,\n    \"max_tokens\": 2000\n}\n\n# Technical documentation configuration\ntechnical_config = {\n    \"temperature\": 0.3,\n    \"top_p\": 0.7,\n    \"top_k\": 50,\n    \"max_tokens\": 1000\n}\n\n# Code generation configuration\ncode_config = {\n    \"temperature\": 0.1,\n    \"top_p\": 0.5,\n    \"top_k\": 25,\n    \"max_tokens\": 1500\n}\n```\n\n#### üí° Use Case Optimization\n\n| Use Case | Temperature | Top-p | Top-k | Rationale |\n|----------|-------------|-------|-------|-----------|\n| **Code Generation** | 0.1-0.3 | 0.5-0.7 | 25-50 | Precision over creativity |\n| **Creative Writing** | 0.7-0.9 | 0.8-0.95 | 100-200 | Balance creativity and coherence |\n| **Technical Docs** | 0.2-0.4 | 0.6-0.8 | 40-80 | Accurate, consistent information |\n| **Chat/Conversation** | 0.5-0.7 | 0.7-0.9 | 80-150 | Natural, engaging responses |\n\n#### üíé Dynamic Parameter Adjustment\n\n```python\ndef adaptive_parameters(task_type, complexity_level):\n    base_configs = {\n        'creative': {'temp': 0.8, 'top_p': 0.9},\n        'analytical': {'temp': 0.3, 'top_p': 0.7},\n        'conversational': {'temp': 0.6, 'top_p': 0.8}\n    }\n    \n    config = base_configs[task_type]\n    \n    # Adjust for complexity\n    if complexity_level == 'high':\n        config['temp'] -= 0.1\n        config['max_tokens'] = 2000\n    \n    return config\n```\n\n#### ‚ö†Ô∏è Parameter Tuning Best Practices\n\n- **Start Conservative**: Begin with lower temperature values\n- **Test Iteratively**: Small parameter changes can have large effects\n- **Monitor Quality**: Track output quality metrics across parameter changes"
      },
      {
        "title": "Response Handling and Error Management",
        "content": "### Response Handling and Error Management\n\nRobust response handling and comprehensive error management ensure reliable application performance when integrating with Bedrock APIs. This section covers parsing responses, implementing retry mechanisms, and gracefully handling various failure scenarios.\n\n#### üîß Response Processing Patterns\n\n```python\nimport json\nimport time\nfrom botocore.exceptions import ClientError\n\ndef process_bedrock_response(response):\n    \"\"\"Safely process Bedrock API response\"\"\"\n    try:\n        response_body = json.loads(response['body'].read())\n        \n        # Handle different model response formats\n        if 'content' in response_body:  # Claude format\n            return response_body['content'][0]['text']\n        elif 'generation' in response_body:  # Other models\n            return response_body['generation']\n        else:\n            raise ValueError(f\"Unexpected response format: {response_body}\")\n            \n    except (json.JSONDecodeError, KeyError) as e:\n        raise ValueError(f\"Failed to parse response: {e}\")\n```\n\n#### ‚ö†Ô∏è Comprehensive Error Handling\n\n```python\ndef robust_bedrock_call(model_id, body, max_retries=3):\n    \"\"\"Bedrock call with comprehensive error handling\"\"\"\n    \n    for attempt in range(max_retries):\n        try:\n            response = bedrock.invoke_model(\n                modelId=model_id,\n                body=body\n            )\n            return process_bedrock_response(response)\n            \n        except ClientError as e:\n            error_code = e.response['Error']['Code']\n            \n            if error_code == 'ThrottlingException':\n                # Exponential backoff for throttling\n                wait_time = (2 ** attempt) * 1\n                print(f\"Rate limited. Waiting {wait_time}s before retry {attempt + 1}\")\n                time.sleep(wait_time)\n                continue\n                \n            elif error_code == 'ValidationException':\n                # Don't retry validation errors\n                raise ValueError(f\"Invalid request: {e.response['Error']['Message']}\")\n                \n            elif error_code == 'ServiceUnavailableException':\n                # Retry service unavailable\n                time.sleep(2 ** attempt)\n                continue\n                \n            else:\n                raise Exception(f\"Bedrock API error: {error_code} - {e.response['Error']['Message']}\")\n                \n        except Exception as e:\n            if attempt == max_retries - 1:\n                raise Exception(f\"Failed after {max_retries} attempts: {str(e)}\")\n            time.sleep(1)\n    \n    raise Exception(f\"Max retries ({max_retries}) exceeded\")\n```\n\n#### üí° Rate Limit Management\n\n```python\nclass RateLimiter:\n    def __init__(self, calls_per_minute=60):\n        self.calls_per_minute = calls_per_minute\n        self.calls = []\n    \n    def wait_if_needed(self):\n        now = time.time()\n        # Remove calls older than 1 minute\n        self.calls = [call_time for call_time in self.calls if now - call_time < 60]\n        \n        if len(self.calls) >= self.calls_per_minute:\n            sleep_time = 60 - (now - self.calls[0])\n            time.sleep(sleep_time)\n        \n        self.calls.append(now)\n```\n\n#### ‚úÖ Production-Ready Implementation\n\n```python\nrate_limiter = RateLimiter(calls_per_minute=50)\n\ndef production_text_generation(prompt):\n    rate_limiter.wait_if_needed()\n    \n    try:\n        return robust_bedrock_call(\n            \"anthropic.claude-3-sonnet-20240229-v1:0\",\n            json.dumps({\n                \"anthropic_version\": \"bedrock-2023-05-31\",\n                \"max_tokens\": 1000,\n                \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n            })\n        )\n    except Exception as e:\n        # Log error and return graceful fallback\n        print(f\"Generation failed: {e}\")\n        return \"I apologize, but I'm unable to process your request at the moment.\"\n```"
      }
    ]
  },
  "3": {
    "sections": [
      {
        "title": "Multimodal Models Overview",
        "content": "### Multimodal Models Overview\n\n**Multimodal AI** represents a significant advancement in artificial intelligence, enabling models to process and understand multiple types of input simultaneously - text, images, audio, and video. These **vision-language models** bridge the gap between textual understanding and visual perception.\n\n#### Core Capabilities\n\n**Vision-Language Models** excel at:\n- **Image Description**: Generating detailed textual descriptions of visual content\n- **Visual Question Answering (VQA)**: Answering questions about image content\n- **Optical Character Recognition (OCR)**: Extracting text from images\n- **Object Detection**: Identifying and locating objects within images\n- **Scene Understanding**: Analyzing context, relationships, and activities\n\n#### Bedrock's Multimodal Ecosystem\n\nüéì **Available Models:**\n- **Claude 3 Vision**: Advanced vision-language understanding\n- **Stability AI**: Image generation and editing\n- **Amazon Titan**: Multimodal embeddings\n\n#### Real-World Applications\n\n**Business Use Cases:**\n- **Content Moderation**: Automatically analyzing user-uploaded images\n- **Document Processing**: Extracting information from scanned documents\n- **E-commerce**: Product catalog automation and visual search\n- **Healthcare**: Medical image analysis and reporting\n- **Education**: Interactive learning with visual content\n\n```python\n# Example: Basic multimodal workflow\nimport boto3\n\nbedrock = boto3.client('bedrock-runtime')\n\n# Process image + text query\nresponse = bedrock.invoke_model(\n    modelId='anthropic.claude-3-sonnet-20240229-v1:0',\n    body={\n        \"anthropic_version\": \"bedrock-2023-05-31\",\n        \"messages\": [{\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"What's in this image?\"},\n                {\"type\": \"image\", \"source\": {\"type\": \"base64\", \"media_type\": \"image/jpeg\", \"data\": image_data}}\n            ]\n        }]\n    }\n)\n```\n\nüí° **Key Insight**: Multimodal models enable applications to understand context that spans multiple media types, creating more natural and comprehensive AI interactions.\n\n‚úÖ **Best Practice**: Always consider the relationship between text and visual elements when designing multimodal applications for optimal results."
      },
      {
        "title": "Image Analysis with Claude Vision",
        "content": "### Image Analysis with Claude Vision\n\n**Claude Vision** brings advanced visual understanding capabilities to Bedrock, enabling sophisticated image analysis tasks through natural language interfaces.\n\n#### Core Vision Capabilities\n\n**Image Understanding Tasks:**\n- **Detailed Descriptions**: Comprehensive scene analysis\n- **OCR and Text Extraction**: Reading text from images\n- **Visual Question Answering**: Context-aware responses\n- **Chart and Graph Analysis**: Data interpretation\n- **Document Analysis**: Form and table processing\n\n#### Implementation Examples\n\n```python\nimport boto3\nimport base64\nimport json\n\ndef analyze_image_with_claude(image_path, question):\n    bedrock = boto3.client('bedrock-runtime')\n    \n    # Encode image to base64\n    with open(image_path, \"rb\") as image_file:\n        image_data = base64.b64encode(image_file.read()).decode('utf-8')\n    \n    body = {\n        \"anthropic_version\": \"bedrock-2023-05-31\",\n        \"max_tokens\": 1000,\n        \"messages\": [{\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": question},\n                {\n                    \"type\": \"image\",\n                    \"source\": {\n                        \"type\": \"base64\",\n                        \"media_type\": \"image/jpeg\",\n                        \"data\": image_data\n                    }\n                }\n            ]\n        }]\n    }\n    \n    response = bedrock.invoke_model(\n        modelId='anthropic.claude-3-sonnet-20240229-v1:0',\n        body=json.dumps(body)\n    )\n    \n    return json.loads(response['body'].read())\n\n# Usage examples\nresult = analyze_image_with_claude(\n    \"receipt.jpg\", \n    \"Extract all items and prices from this receipt\"\n)\n```\n\n#### Advanced Vision Tasks\n\nüîß **Document Processing:**\n```python\n# OCR with structured output\nquestion = \"\"\"Extract the following information from this form:\n- Name\n- Date\n- Amount\n- Signature status\nFormat as JSON.\"\"\"\n\nresult = analyze_image_with_claude(\"form.pdf\", question)\n```\n\n‚ö†Ô∏è **Important Considerations:**\n- **Image Quality**: Higher resolution images yield better results\n- **File Size Limits**: Maximum 20MB per image\n- **Supported Formats**: JPEG, PNG, GIF, WebP\n- **Token Usage**: Images consume significant tokens\n\nüíé **Pro Tip**: Use specific, detailed prompts for better accuracy. Instead of \"What's in this image?\", try \"List all visible products with their prices and locations in this retail photo.\""
      },
      {
        "title": "Image Generation with Stability AI",
        "content": "### Image Generation with Stability AI\n\n**Stability AI models** on Bedrock provide powerful image generation capabilities through **Stable Diffusion**, enabling creative visual content creation from text descriptions.\n\n#### Available Models\n\n**Stability AI Options:**\n- **SDXL 1.0**: High-quality, versatile image generation\n- **Stable Diffusion 1.6**: Fast, efficient generation\n- **Image-to-Image**: Transform existing images\n\n#### Basic Image Generation\n\n```python\nimport boto3\nimport json\nimport base64\nfrom PIL import Image\nimport io\n\ndef generate_image(prompt, style_preset=\"photographic\"):\n    bedrock = boto3.client('bedrock-runtime')\n    \n    body = {\n        \"text_prompts\": [\n            {\"text\": prompt, \"weight\": 1.0}\n        ],\n        \"cfg_scale\": 7,\n        \"steps\": 30,\n        \"seed\": 12345,\n        \"width\": 1024,\n        \"height\": 1024,\n        \"style_preset\": style_preset\n    }\n    \n    response = bedrock.invoke_model(\n        modelId='stability.stable-diffusion-xl-v1',\n        body=json.dumps(body)\n    )\n    \n    response_body = json.loads(response['body'].read())\n    image_data = response_body['artifacts'][0]['base64']\n    \n    # Convert to PIL Image\n    image_bytes = base64.b64decode(image_data)\n    return Image.open(io.BytesIO(image_bytes))\n\n# Generate an image\nimage = generate_image(\n    \"A serene mountain landscape at sunset, photorealistic style\",\n    style_preset=\"photographic\"\n)\nimage.save(\"generated_landscape.png\")\n```\n\n#### Advanced Prompt Engineering\n\nüéì **Effective Prompt Structure:**\n```python\n# Detailed prompt with quality modifiers\nadvanced_prompt = \"\"\"\nA professional headshot of a businesswoman, \ncorporate attire, confident smile, \nstudio lighting, high resolution, \ncommercial photography style, \nsharp focus, neutral background\n\"\"\"\n\n# Negative prompts for better results\nbody = {\n    \"text_prompts\": [\n        {\"text\": advanced_prompt, \"weight\": 1.0},\n        {\"text\": \"blurry, low quality, distorted\", \"weight\": -1.0}\n    ],\n    \"cfg_scale\": 10,  # Higher for more prompt adherence\n    \"steps\": 50       # More steps for quality\n}\n```\n\n#### Style Presets and Parameters\n\n| Parameter | Purpose | Range |\n|-----------|---------|-------|\n| `cfg_scale` | Prompt adherence | 1-20 |\n| `steps` | Generation quality | 10-150 |\n| `seed` | Reproducibility | 0-4294967295 |\n\nüí° **Style Presets**: `photographic`, `digital-art`, `comic-book`, `fantasy-art`, `line-art`, `analog-film`, `neon-punk`, `isometric`\n\n‚ö†Ô∏è **Cost Consideration**: Higher steps and resolution increase generation costs. Start with default settings and optimize based on results."
      },
      {
        "title": "Image Processing and Optimization",
        "content": "### Image Processing and Optimization\n\n**Efficient image processing** is crucial for multimodal applications, ensuring optimal performance while managing API limits and costs.\n\n#### Image Format Handling\n\n```python\nfrom PIL import Image\nimport base64\nimport io\n\ndef optimize_image_for_bedrock(image_path, max_size_mb=20, target_width=1024):\n    \"\"\"Optimize image for Bedrock API limits\"\"\"\n    with Image.open(image_path) as img:\n        # Convert to RGB if necessary\n        if img.mode in ('RGBA', 'LA', 'P'):\n            img = img.convert('RGB')\n        \n        # Resize if too large\n        if img.width > target_width:\n            ratio = target_width / img.width\n            new_height = int(img.height * ratio)\n            img = img.resize((target_width, new_height), Image.LANCZOS)\n        \n        # Compress to meet size limit\n        quality = 95\n        while True:\n            buffer = io.BytesIO()\n            img.save(buffer, format='JPEG', quality=quality, optimize=True)\n            size_mb = len(buffer.getvalue()) / (1024 * 1024)\n            \n            if size_mb <= max_size_mb or quality <= 10:\n                break\n            quality -= 5\n        \n        return base64.b64encode(buffer.getvalue()).decode('utf-8')\n\n# Usage\noptimized_image = optimize_image_for_bedrock(\"large_image.png\")\n```\n\n#### Batch Processing Workflow\n\n```python\nimport asyncio\nimport aiohttp\nfrom concurrent.futures import ThreadPoolExecutor\n\nclass ImageProcessor:\n    def __init__(self, bedrock_client):\n        self.bedrock = bedrock_client\n        self.executor = ThreadPoolExecutor(max_workers=5)\n    \n    async def process_images_batch(self, image_paths, prompt):\n        \"\"\"Process multiple images concurrently\"\"\"\n        tasks = []\n        \n        for path in image_paths:\n            task = asyncio.get_event_loop().run_in_executor(\n                self.executor,\n                self.process_single_image,\n                path,\n                prompt\n            )\n            tasks.append(task)\n        \n        results = await asyncio.gather(*tasks, return_exceptions=True)\n        return results\n    \n    def process_single_image(self, image_path, prompt):\n        \"\"\"Process single image with error handling\"\"\"\n        try:\n            optimized_data = optimize_image_for_bedrock(image_path)\n            # Process with Claude Vision\n            return self.analyze_with_claude(optimized_data, prompt)\n        except Exception as e:\n            return {\"error\": str(e), \"image\": image_path}\n```\n\n#### Performance Optimization\n\nüîß **Memory Management:**\n```python\ndef efficient_image_processing():\n    # Use context managers for automatic cleanup\n    with Image.open('large_image.jpg') as img:\n        # Process image\n        processed = img.resize((512, 512))\n        # Image automatically closed after context\n        return processed\n```\n\nüìä **Size Guidelines:**\n\n| Use Case | Recommended Size | Quality Setting |\n|----------|-----------------|----------------|\n| OCR/Text | 1024px width | High (90-95) |\n| General Analysis | 768px width | Medium (80-85) |\n| Thumbnails | 512px width | Low (70-75) |\n\n‚ö†Ô∏è **API Limits:**\n- **Maximum file size**: 20MB\n- **Supported formats**: JPEG, PNG, GIF, WebP\n- **Rate limits**: Monitor request frequency\n\nüíé **Best Practice**: Implement image caching to avoid reprocessing identical images, reducing costs and improving response times."
      },
      {
        "title": "Building Multimodal Applications",
        "content": "### Building Multimodal Applications\n\n**Multimodal applications** combine text and visual inputs to create comprehensive AI-powered solutions that understand and generate content across multiple media types.\n\n#### Application Architecture\n\n```python\nclass MultimodalApp:\n    def __init__(self):\n        self.bedrock = boto3.client('bedrock-runtime')\n        self.image_processor = ImageProcessor(self.bedrock)\n    \n    async def analyze_document(self, image_path, questions):\n        \"\"\"Complete document analysis workflow\"\"\"\n        # Step 1: Optimize image\n        optimized_image = optimize_image_for_bedrock(image_path)\n        \n        # Step 2: Extract text and analyze\n        extraction_prompt = \"\"\"Extract all text from this document and \n        structure it logically. Include headers, paragraphs, and any tabular data.\"\"\"\n        \n        text_content = await self.vision_analysis(\n            optimized_image, extraction_prompt\n        )\n        \n        # Step 3: Answer specific questions\n        insights = {}\n        for question in questions:\n            combined_prompt = f\"\"\"\n            Based on this document image and extracted text:\n            {text_content}\n            \n            Question: {question}\n            \"\"\"\n            \n            insights[question] = await self.vision_analysis(\n                optimized_image, combined_prompt\n            )\n        \n        return {\n            \"extracted_text\": text_content,\n            \"insights\": insights,\n            \"image_metadata\": self.get_image_metadata(image_path)\n        }\n```\n\n#### E-commerce Product Analyzer\n\n```python\nclass ProductAnalyzer:\n    def __init__(self, bedrock_client):\n        self.bedrock = bedrock_client\n    \n    async def analyze_product_image(self, image_path):\n        \"\"\"Complete product analysis and catalog generation\"\"\"\n        \n        analysis_tasks = [\n            self.extract_product_details(image_path),\n            self.generate_marketing_copy(image_path),\n            self.suggest_categories(image_path),\n            self.generate_product_variants(image_path)\n        ]\n        \n        results = await asyncio.gather(*analysis_tasks)\n        \n        return {\n            \"product_details\": results[0],\n            \"marketing_copy\": results[1],\n            \"categories\": results[2],\n            \"variants\": results[3]\n        }\n    \n    async def extract_product_details(self, image_path):\n        prompt = \"\"\"\n        Analyze this product image and extract:\n        - Product name and type\n        - Key features and specifications\n        - Colors and materials\n        - Estimated price range\n        - Target demographic\n        Format as structured JSON.\n        \"\"\"\n        \n        return await self.vision_analysis(image_path, prompt)\n    \n    async def generate_marketing_copy(self, image_path):\n        prompt = \"\"\"\n        Create compelling marketing copy for this product:\n        - Catchy headline (under 10 words)\n        - Product description (2-3 sentences)\n        - Key selling points (3-5 bullets)\n        - Call-to-action\n        \"\"\"\n        \n        return await self.vision_analysis(image_path, prompt)\n```\n\n#### Interactive Visual Assistant\n\n```python\nclass VisualAssistant:\n    def __init__(self):\n        self.conversation_history = []\n        self.current_images = {}\n    \n    async def chat_with_images(self, user_input, images=None):\n        \"\"\"Continuous conversation with visual context\"\"\"\n        \n        if images:\n            self.current_images.update(images)\n        \n        # Build context with conversation history and images\n        messages = self.build_message_context(user_input)\n        \n        response = await self.get_claude_response(messages)\n        \n        # Update conversation history\n        self.conversation_history.append({\n            \"user\": user_input,\n            \"assistant\": response,\n            \"images\": list(images.keys()) if images else []\n        })\n        \n        return response\n    \n    def build_message_context(self, current_input):\n        \"\"\"Build comprehensive message context\"\"\"\n        messages = []\n        \n        # Add conversation history\n        for exchange in self.conversation_history[-3:]:  # Keep recent context\n            messages.append({\n                \"role\": \"user\",\n                \"content\": exchange[\"user\"]\n            })\n            messages.append({\n                \"role\": \"assistant\",\n                \"content\": exchange[\"assistant\"]\n            })\n        \n        # Add current input with images\n        content = [{\"type\": \"text\", \"text\": current_input}]\n        \n        for img_name, img_data in self.current_images.items():\n            content.append({\n                \"type\": \"image\",\n                \"source\": {\n                    \"type\": \"base64\",\n                    \"media_type\": \"image/jpeg\",\n                    \"data\": img_data\n                }\n            })\n        \n        messages.append({\"role\": \"user\", \"content\": content})\n        return messages\n\n# Usage example\nassistant = VisualAssistant()\n\n# Start conversation with image\nresponse = await assistant.chat_with_images(\n    \"What do you see in this image?\",\n    images={\"photo1\": base64_image_data}\n)\n\n# Continue conversation\nfollow_up = await assistant.chat_with_images(\n    \"Can you suggest improvements to the lighting?\"\n)\n```\n\nüéì **Application Patterns:**\n- **Document Intelligence**: OCR + Analysis + Insights\n- **Content Creation**: Image Analysis + Text Generation\n- **Visual Search**: Image Understanding + Matching\n- **Quality Control**: Image Analysis + Decision Making\n\n‚úÖ **Success Factors:**\n- **Context Management**: Maintain conversation and image context\n- **Error Handling**: Graceful degradation for processing failures\n- **Performance**: Optimize for response time and cost\n- **User Experience**: Intuitive multimodal interactions"
      }
    ]
  },
  "4": {
    "sections": [
      {
        "title": "Bedrock Knowledge Bases and RAG",
        "content": "### Bedrock Knowledge Bases and RAG\n\n**Retrieval Augmented Generation (RAG)** transforms your Bedrock models by grounding responses in your specific data sources. Amazon Bedrock Knowledge Bases provides a managed solution for implementing RAG patterns without complex infrastructure management.\n\n#### Setting Up Knowledge Bases\n\n```python\nimport boto3\n\nbedrock_agent = boto3.client('bedrock-agent')\n\n# Create a Knowledge Base\nknowledge_base = bedrock_agent.create_knowledge_base(\n    name='company-docs-kb',\n    description='Internal company documentation',\n    roleArn='arn:aws:iam::account:role/BedrockKBRole',\n    knowledgeBaseConfiguration={\n        'type': 'VECTOR',\n        'vectorKnowledgeBaseConfiguration': {\n            'embeddingModelArn': 'arn:aws:bedrock:us-east-1::foundation-model/amazon.titan-embed-text-v1'\n        }\n    },\n    storageConfiguration={\n        'type': 'OPENSEARCH_SERVERLESS',\n        'opensearchServerlessConfiguration': {\n            'collectionArn': 'arn:aws:aoss:region:account:collection/kb-collection',\n            'vectorIndexName': 'company-docs-index',\n            'fieldMapping': {\n                'vectorField': 'vector',\n                'textField': 'text',\n                'metadataField': 'metadata'\n            }\n        }\n    }\n)\n```\n\n#### Document Ingestion Process\n\nüîß **Supported formats**: PDF, TXT, MD, HTML, DOC, CSV\n\n```python\n# Create data source for S3 bucket\ndata_source = bedrock_agent.create_data_source(\n    knowledgeBaseId=knowledge_base['knowledgeBase']['knowledgeBaseId'],\n    name='s3-documents',\n    dataSourceConfiguration={\n        'type': 'S3',\n        's3Configuration': {\n            'bucketArn': 'arn:aws:s3:::my-documents-bucket',\n            'inclusionPrefixes': ['docs/', 'manuals/']\n        }\n    },\n    vectorIngestionConfiguration={\n        'chunkingConfiguration': {\n            'chunkingStrategy': 'FIXED_SIZE',\n            'fixedSizeChunkingConfiguration': {\n                'maxTokens': 300,\n                'overlapPercentage': 20\n            }\n        }\n    }\n)\n\n# Start ingestion job\ningestion_job = bedrock_agent.start_ingestion_job(\n    knowledgeBaseId=knowledge_base_id,\n    dataSourceId=data_source['dataSource']['dataSourceId']\n)\n```\n\n#### Querying with RAG\n\n```python\nbedrock_runtime = boto3.client('bedrock-agent-runtime')\n\nresponse = bedrock_runtime.retrieve_and_generate(\n    input={\n        'text': 'What are our company vacation policies?'\n    },\n    retrieveAndGenerateConfiguration={\n        'type': 'KNOWLEDGE_BASE',\n        'knowledgeBaseConfiguration': {\n            'knowledgeBaseId': knowledge_base_id,\n            'modelArn': 'arn:aws:bedrock:us-east-1::foundation-model/anthropic.claude-3-sonnet-20240229-v1:0'\n        }\n    }\n)\n\nprint(response['output']['text'])\n```\n\nüí° **Pro tip**: Use metadata filtering to restrict searches to specific document types or time periods, improving relevance and reducing costs."
      },
      {
        "title": "Model Customization Options",
        "content": "### Model Customization Options\n\n**Model customization** in Bedrock allows you to adapt foundation models to your specific domain, style, or use case. Understanding when and how to apply different customization techniques is crucial for optimal performance.\n\n#### Fine-tuning vs. Continued Pre-training\n\n| **Fine-tuning** | **Continued Pre-training** |\n|-----------------|---------------------------|\n| Task-specific adaptation | Domain knowledge injection |\n| Smaller datasets (100s-1000s examples) | Large datasets (GBs of text) |\n| Changes behavior/style | Adds new knowledge |\n| Faster, cheaper | Slower, more expensive |\n\n#### Fine-tuning Implementation\n\n```python\nimport boto3\n\nbedrock = boto3.client('bedrock')\n\n# Prepare training data (JSONL format)\ntraining_data = {\n    \"prompt\": \"Classify the sentiment of this review: 'The product exceeded expectations'\",\n    \"completion\": \"Positive\"\n}\n\n# Create fine-tuning job\nfine_tune_job = bedrock.create_model_customization_job(\n    jobName='sentiment-classifier-v1',\n    customModelName='company-sentiment-model',\n    roleArn='arn:aws:iam::account:role/BedrockCustomizationRole',\n    baseModelIdentifier='amazon.titan-text-express-v1',\n    trainingDataConfig={\n        's3Uri': 's3://my-training-bucket/sentiment-data.jsonl'\n    },\n    validationDataConfig={\n        's3Uri': 's3://my-training-bucket/validation-data.jsonl'\n    },\n    hyperParameters={\n        'epochCount': '3',\n        'batchSize': '1',\n        'learningRate': '0.0001'\n    }\n)\n```\n\n#### When to Use Each Approach\n\nüéì **Fine-tuning scenarios**:\n- Customer service chatbots with specific tone\n- Code generation for particular frameworks\n- Content summarization with specific formats\n- Classification tasks with domain-specific categories\n\nüîß **Continued pre-training scenarios**:\n- Medical/legal domain expertise\n- Industry-specific terminology\n- Regional language variations\n- Proprietary knowledge integration\n\n#### Monitoring Custom Models\n\n```python\n# Check job status\njob_status = bedrock.get_model_customization_job(\n    jobIdentifier=fine_tune_job['jobArn']\n)\n\nprint(f\"Status: {job_status['status']}\")\nprint(f\"Training metrics: {job_status.get('trainingMetrics', {})}\")\n\n# Use custom model\nif job_status['status'] == 'Completed':\n    custom_model_arn = job_status['outputModelArn']\n    \n    response = bedrock_runtime.invoke_model(\n        modelId=custom_model_arn,\n        body=json.dumps({\n            \"inputText\": \"Analyze this customer feedback\",\n            \"textGenerationConfig\": {\n                \"temperature\": 0.1,\n                \"maxTokenCount\": 100\n            }\n        })\n    )\n```\n\n‚ö†Ô∏è **Important considerations**:\n- Custom models incur additional hosting costs\n- Evaluate against base models to ensure improvement\n- Consider data privacy implications\n- Plan for model versioning and updates\n\nüí° **Best practice**: Start with prompt engineering and RAG before investing in custom training."
      },
      {
        "title": "Vector Database Integration",
        "content": "### Vector Database Integration\n\n**External vector databases** provide advanced search capabilities, scalability, and flexibility beyond Bedrock's built-in Knowledge Bases. Integrating with specialized vector databases enables sophisticated retrieval strategies and hybrid search approaches.\n\n#### Popular Vector Database Options\n\n| **Database** | **Best For** | **Key Features** |\n|--------------|--------------|------------------|\n| **Pinecone** | Managed simplicity | Auto-scaling, metadata filtering |\n| **Weaviate** | Hybrid search | Graph capabilities, multi-modal |\n| **Chroma** | Local development | Open-source, lightweight |\n| **FAISS** | High performance | Facebook's optimized search |\n\n#### Pinecone Integration Example\n\n```python\nimport pinecone\nimport boto3\nfrom sentence_transformers import SentenceTransformer\n\n# Initialize Pinecone\npinecone.init(\n    api_key=\"your-api-key\",\n    environment=\"us-west1-gcp-free\"\n)\n\n# Create index\nindex_name = \"company-knowledge\"\nif index_name not in pinecone.list_indexes():\n    pinecone.create_index(\n        name=index_name,\n        dimension=384,  # sentence-transformers/all-MiniLM-L6-v2\n        metric=\"cosine\"\n    )\n\nindex = pinecone.Index(index_name)\n\n# Embedding model\nencoder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')\n\nclass VectorRAGSystem:\n    def __init__(self, index, encoder, bedrock_client):\n        self.index = index\n        self.encoder = encoder\n        self.bedrock = bedrock_client\n    \n    def add_documents(self, documents):\n        \"\"\"Add documents to vector database\"\"\"\n        vectors = []\n        for i, doc in enumerate(documents):\n            embedding = self.encoder.encode(doc['text']).tolist()\n            vectors.append({\n                'id': f\"doc_{i}\",\n                'values': embedding,\n                'metadata': {\n                    'text': doc['text'],\n                    'source': doc.get('source', ''),\n                    'category': doc.get('category', '')\n                }\n            })\n        \n        self.index.upsert(vectors=vectors)\n    \n    def retrieve_and_generate(self, query, top_k=3):\n        \"\"\"Retrieve relevant docs and generate response\"\"\"\n        # Vector search\n        query_embedding = self.encoder.encode(query).tolist()\n        search_results = self.index.query(\n            vector=query_embedding,\n            top_k=top_k,\n            include_metadata=True\n        )\n        \n        # Extract context\n        context = \"\\n\\n\".join([\n            match['metadata']['text'] \n            for match in search_results['matches']\n        ])\n        \n        # Generate response with Bedrock\n        prompt = f\"\"\"Context information:\n{context}\n\nQuestion: {query}\nAnswer based on the context:\"\"\"\n        \n        response = self.bedrock.invoke_model(\n            modelId=\"anthropic.claude-3-sonnet-20240229-v1:0\",\n            body=json.dumps({\n                \"anthropic_version\": \"bedrock-2023-05-31\",\n                \"messages\": [{\n                    \"role\": \"user\",\n                    \"content\": prompt\n                }],\n                \"max_tokens\": 500\n            })\n        )\n        \n        return json.loads(response['body'].read())\n```\n\n#### Advanced Retrieval Strategies\n\nüîß **Hybrid Search Implementation**:\n\n```python\ndef hybrid_search(self, query, alpha=0.7):\n    \"\"\"Combine vector and keyword search\"\"\"\n    # Vector search\n    vector_results = self.vector_search(query, top_k=10)\n    \n    # Keyword search (using metadata)\n    keyword_results = self.index.query(\n        vector=[0] * 384,  # Dummy vector\n        filter={\n            \"text\": {\"$regex\": f\".*{query}.*\"}\n        },\n        top_k=10\n    )\n    \n    # Combine scores\n    combined_results = self.combine_search_results(\n        vector_results, keyword_results, alpha\n    )\n    \n    return combined_results\n```\n\nüí° **Performance optimization tips**:\n- Use appropriate embedding dimensions (384-1536)\n- Implement caching for frequent queries\n- Batch upsert operations\n- Monitor search latency and accuracy\n\n‚úÖ **Integration benefits**:\n- Advanced filtering capabilities\n- Real-time updates\n- Custom similarity metrics\n- Multi-tenant isolation"
      },
      {
        "title": "Prompt Templates and Workflows",
        "content": "### Prompt Templates and Workflows\n\n**Structured prompt templates** and **multi-step workflows** enable consistent, maintainable AI applications. By creating reusable components, you can build complex reasoning chains while maintaining quality and reducing development time.\n\n#### Creating Reusable Prompt Templates\n\n```python\nfrom string import Template\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Any\n\n@dataclass\nclass PromptTemplate:\n    name: str\n    template: str\n    required_variables: List[str]\n    model_config: Dict[str, Any]\n    \n    def render(self, **kwargs):\n        \"\"\"Render template with provided variables\"\"\"\n        missing = [var for var in self.required_variables if var not in kwargs]\n        if missing:\n            raise ValueError(f\"Missing required variables: {missing}\")\n        \n        return Template(self.template).substitute(**kwargs)\n\n# Define template library\nTEMPLATES = {\n    \"code_review\": PromptTemplate(\n        name=\"Code Review Assistant\",\n        template=\"\"\"You are an expert code reviewer. Analyze this code for:\n- Security vulnerabilities\n- Performance issues\n- Best practices\n- Code style\n\nCode Language: $language\nCode to review:\n```$language\n$code\n```\n\nProvide specific, actionable feedback:\"\"\",\n        required_variables=[\"language\", \"code\"],\n        model_config={\"temperature\": 0.1, \"max_tokens\": 1000}\n    ),\n    \n    \"meeting_summarizer\": PromptTemplate(\n        name=\"Meeting Summary Generator\",\n        template=\"\"\"Summarize this meeting transcript into:\n1. Key decisions made\n2. Action items assigned\n3. Important discussion points\n4. Next meeting date/agenda\n\nMeeting: $meeting_title\nDate: $meeting_date\nAttendees: $attendees\n\nTranscript:\n$transcript\n\nSummary:\"\"\",\n        required_variables=[\"meeting_title\", \"meeting_date\", \"attendees\", \"transcript\"],\n        model_config={\"temperature\": 0.3, \"max_tokens\": 800}\n    )\n}\n```\n\n#### Multi-Step Workflow Engine\n\n```python\nimport asyncio\nfrom enum import Enum\nfrom typing import Callable, Optional\n\nclass StepType(Enum):\n    BEDROCK_INVOKE = \"bedrock_invoke\"\n    KNOWLEDGE_BASE = \"knowledge_base\"\n    CUSTOM_FUNCTION = \"custom_function\"\n    CONDITIONAL = \"conditional\"\n\nclass WorkflowStep:\n    def __init__(self, name: str, step_type: StepType, \n                 config: Dict[str, Any], next_step: Optional[str] = None):\n        self.name = name\n        self.step_type = step_type\n        self.config = config\n        self.next_step = next_step\n\nclass WorkflowEngine:\n    def __init__(self, bedrock_client):\n        self.bedrock = bedrock_client\n        self.steps = {}\n        self.context = {}\n    \n    def add_step(self, step: WorkflowStep):\n        self.steps[step.name] = step\n    \n    async def execute_workflow(self, start_step: str, initial_context: Dict):\n        self.context.update(initial_context)\n        current_step = start_step\n        \n        while current_step:\n            step = self.steps[current_step]\n            print(f\"üîß Executing step: {step.name}\")\n            \n            if step.step_type == StepType.BEDROCK_INVOKE:\n                await self._execute_bedrock_step(step)\n            elif step.step_type == StepType.KNOWLEDGE_BASE:\n                await self._execute_kb_step(step)\n            elif step.step_type == StepType.CUSTOM_FUNCTION:\n                await self._execute_custom_step(step)\n            elif step.step_type == StepType.CONDITIONAL:\n                current_step = self._execute_conditional_step(step)\n                continue\n            \n            current_step = step.next_step\n        \n        return self.context\n    \n    async def _execute_bedrock_step(self, step: WorkflowStep):\n        template = TEMPLATES[step.config['template']]\n        prompt = template.render(**{k: self.context[k] for k in template.required_variables})\n        \n        response = self.bedrock.invoke_model(\n            modelId=step.config['model_id'],\n            body=json.dumps({\n                \"anthropic_version\": \"bedrock-2023-05-31\",\n                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n                **template.model_config\n            })\n        )\n        \n        result = json.loads(response['body'].read())\n        self.context[step.config['output_key']] = result['content'][0]['text']\n\n# Example: Content Creation Workflow\nworkflow = WorkflowEngine(bedrock_client)\n\n# Step 1: Generate outline\nworkflow.add_step(WorkflowStep(\n    name=\"generate_outline\",\n    step_type=StepType.BEDROCK_INVOKE,\n    config={\n        \"template\": \"content_outliner\",\n        \"model_id\": \"anthropic.claude-3-sonnet-20240229-v1:0\",\n        \"output_key\": \"outline\"\n    },\n    next_step=\"research_topics\"\n))\n\n# Step 2: Research each topic\nworkflow.add_step(WorkflowStep(\n    name=\"research_topics\",\n    step_type=StepType.KNOWLEDGE_BASE,\n    config={\n        \"knowledge_base_id\": \"kb-12345\",\n        \"query_template\": \"Research information about {topic}\",\n        \"output_key\": \"research_data\"\n    },\n    next_step=\"create_content\"\n))\n\n# Step 3: Generate final content\nworkflow.add_step(WorkflowStep(\n    name=\"create_content\",\n    step_type=StepType.BEDROCK_INVOKE,\n    config={\n        \"template\": \"content_writer\",\n        \"model_id\": \"anthropic.claude-3-sonnet-20240229-v1:0\",\n        \"output_key\": \"final_content\"\n    }\n))\n```\n\n#### Template Management System\n\nüíé **Version control for templates**:\n\n```python\nclass TemplateManager:\n    def __init__(self):\n        self.templates = {}\n        self.versions = {}\n    \n    def register_template(self, template: PromptTemplate, version: str = \"1.0\"):\n        key = f\"{template.name}:{version}\"\n        self.templates[key] = template\n        \n        if template.name not in self.versions:\n            self.versions[template.name] = []\n        self.versions[template.name].append(version)\n    \n    def get_template(self, name: str, version: str = \"latest\"):\n        if version == \"latest\":\n            version = max(self.versions[name])\n        return self.templates[f\"{name}:{version}\"]\n```\n\nüéì **Benefits of structured workflows**:\n- Consistent output quality\n- Easy debugging and monitoring\n- Reusable components\n- A/B testing capabilities\n- Error handling and retry logic"
      },
      {
        "title": "Performance Optimization",
        "content": "### Performance Optimization\n\n**Production-ready Bedrock applications** require careful optimization of response times, costs, and accuracy. Understanding performance bottlenecks and implementing strategic optimizations ensures scalable, efficient AI systems.\n\n#### Response Time Optimization\n\nüîß **Caching Strategies**:\n\n```python\nimport redis\nimport hashlib\nimport json\nfrom functools import wraps\nfrom typing import Optional\n\nclass BedrockCache:\n    def __init__(self, redis_host='localhost', redis_port=6379, ttl=3600):\n        self.redis_client = redis.Redis(host=redis_host, port=redis_port, decode_responses=True)\n        self.default_ttl = ttl\n    \n    def cache_key(self, model_id: str, prompt: str, params: dict) -> str:\n        \"\"\"Generate cache key from request parameters\"\"\"\n        content = f\"{model_id}:{prompt}:{json.dumps(params, sort_keys=True)}\"\n        return hashlib.md5(content.encode()).hexdigest()\n    \n    def get_cached_response(self, cache_key: str) -> Optional[dict]:\n        \"\"\"Retrieve cached response\"\"\"\n        cached = self.redis_client.get(cache_key)\n        return json.loads(cached) if cached else None\n    \n    def cache_response(self, cache_key: str, response: dict, ttl: Optional[int] = None):\n        \"\"\"Store response in cache\"\"\"\n        ttl = ttl or self.default_ttl\n        self.redis_client.setex(\n            cache_key,\n            ttl,\n            json.dumps(response, default=str)\n        )\n\ndef cached_bedrock_invoke(cache: BedrockCache, ttl: int = 3600):\n    \"\"\"Decorator for caching Bedrock responses\"\"\"\n    def decorator(func):\n        @wraps(func)\n        def wrapper(model_id: str, prompt: str, **params):\n            # Check cache first\n            cache_key = cache.cache_key(model_id, prompt, params)\n            cached_response = cache.get_cached_response(cache_key)\n            \n            if cached_response:\n                print(f\"‚úÖ Cache hit for {cache_key[:8]}...\")\n                return cached_response\n            \n            # Make actual request\n            print(f\"üîß Cache miss, invoking Bedrock...\")\n            response = func(model_id, prompt, **params)\n            \n            # Cache the response\n            cache.cache_response(cache_key, response, ttl)\n            return response\n        return wrapper\n    return decorator\n\n# Usage\ncache = BedrockCache()\n\n@cached_bedrock_invoke(cache, ttl=1800)  # 30 minutes\ndef invoke_with_cache(model_id: str, prompt: str, **params):\n    return bedrock_runtime.invoke_model(\n        modelId=model_id,\n        body=json.dumps({\n            \"anthropic_version\": \"bedrock-2023-05-31\",\n            \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n            **params\n        })\n    )\n```\n\n#### Cost Optimization Strategies\n\nüí° **Model Selection Matrix**:\n\n| **Use Case** | **Recommended Model** | **Cost/1K Tokens** | **Speed** |\n|--------------|----------------------|-------------------|----------|\n| **Simple Q&A** | Claude 3 Haiku | $0.00025 | Fast |\n| **Complex reasoning** | Claude 3 Sonnet | $0.003 | Medium |\n| **Code generation** | Claude 3.5 Sonnet | $0.003 | Medium |\n| **Creative writing** | Claude 3 Opus | $0.015 | Slow |\n\n```python\nclass IntelligentModelRouter:\n    def __init__(self):\n        self.model_costs = {\n            \"claude-3-haiku\": 0.00025,\n            \"claude-3-sonnet\": 0.003,\n            \"claude-3-opus\": 0.015\n        }\n        self.complexity_thresholds = {\n            \"simple\": (\"claude-3-haiku\", 100),\n            \"medium\": (\"claude-3-sonnet\", 500),\n            \"complex\": (\"claude-3-opus\", 1000)\n        }\n    \n    def estimate_complexity(self, prompt: str) -> str:\n        \"\"\"Estimate prompt complexity based on keywords and length\"\"\"\n        complex_keywords = [\"analyze\", \"explain\", \"reasoning\", \"detailed\", \"comprehensive\"]\n        simple_keywords = [\"what\", \"who\", \"when\", \"yes\", \"no\"]\n        \n        word_count = len(prompt.split())\n        complex_score = sum(1 for word in complex_keywords if word in prompt.lower())\n        simple_score = sum(1 for word in simple_keywords if word in prompt.lower())\n        \n        if word_count > 100 or complex_score > simple_score:\n            return \"complex\" if word_count > 200 else \"medium\"\n        return \"simple\"\n    \n    def route_request(self, prompt: str, max_cost: float = 0.01):\n        \"\"\"Route to appropriate model based on complexity and budget\"\"\"\n        complexity = self.estimate_complexity(prompt)\n        model, max_tokens = self.complexity_thresholds[complexity]\n        \n        estimated_cost = self.model_costs[model] * max_tokens\n        if estimated_cost > max_cost:\n            # Downgrade to cheaper model\n            model = \"claude-3-haiku\"\n            max_tokens = min(max_tokens, int(max_cost / self.model_costs[model]))\n        \n        return model, max_tokens\n\n# Usage\nrouter = IntelligentModelRouter()\nmodel, tokens = router.route_request(\"What's the weather today?\")  # Routes to Haiku\nmodel, tokens = router.route_request(\"Analyze the complex implications of quantum computing\")  # Routes to Sonnet/Opus\n```\n\n#### Accuracy Improvements\n\n‚ö†Ô∏è **Quality Monitoring System**:\n\n```python\nfrom dataclasses import dataclass\nfrom typing import List, Dict\nimport numpy as np\n\n@dataclass\nclass QualityMetrics:\n    response_id: str\n    relevance_score: float\n    coherence_score: float\n    factual_accuracy: float\n    user_satisfaction: float\n    \n    @property\n    def overall_score(self) -> float:\n        return np.mean([self.relevance_score, self.coherence_score, \n                       self.factual_accuracy, self.user_satisfaction])\n\nclass QualityMonitor:\n    def __init__(self, threshold=0.7):\n        self.quality_threshold = threshold\n        self.metrics_history = []\n    \n    def evaluate_response(self, prompt: str, response: str, \n                         ground_truth: str = None) -> QualityMetrics:\n        \"\"\"Evaluate response quality using multiple metrics\"\"\"\n        # Simplified scoring - in production, use more sophisticated methods\n        relevance = self._calculate_relevance(prompt, response)\n        coherence = self._calculate_coherence(response)\n        accuracy = self._calculate_accuracy(response, ground_truth) if ground_truth else 0.8\n        \n        metrics = QualityMetrics(\n            response_id=hashlib.md5(response.encode()).hexdigest()[:8],\n            relevance_score=relevance,\n            coherence_score=coherence,\n            factual_accuracy=accuracy,\n            user_satisfaction=0.0  # To be updated by user feedback\n        )\n        \n        self.metrics_history.append(metrics)\n        \n        if metrics.overall_score < self.quality_threshold:\n            print(f\"‚ö†Ô∏è Quality alert: Score {metrics.overall_score:.2f} below threshold\")\n            self._trigger_quality_improvement(prompt, response)\n        \n        return metrics\n    \n    def _trigger_quality_improvement(self, prompt: str, response: str):\n        \"\"\"Implement quality improvement strategies\"\"\"\n        # Strategy 1: Retry with different model\n        # Strategy 2: Enhance prompt with examples\n        # Strategy 3: Use RAG for additional context\n        pass\n    \n    def get_quality_trends(self) -> Dict[str, float]:\n        \"\"\"Calculate quality trends over time\"\"\"\n        if not self.metrics_history:\n            return {}\n        \n        recent = self.metrics_history[-100:]  # Last 100 responses\n        return {\n            \"avg_relevance\": np.mean([m.relevance_score for m in recent]),\n            \"avg_coherence\": np.mean([m.coherence_score for m in recent]),\n            \"avg_accuracy\": np.mean([m.factual_accuracy for m in recent]),\n            \"quality_trend\": \"improving\" if len(recent) > 10 and \n                           recent[-5:] > recent[:5] else \"stable\"\n        }\n```\n\n#### Production Monitoring Dashboard\n\n```python\nimport boto3\nimport time\nfrom collections import defaultdict, deque\n\nclass BedrockMonitor:\n    def __init__(self):\n        self.cloudwatch = boto3.client('cloudwatch')\n        self.metrics = defaultdict(deque)\n        self.start_time = time.time()\n    \n    def log_request(self, model_id: str, tokens: int, latency: float, cost: float):\n        \"\"\"Log request metrics\"\"\"\n        timestamp = time.time()\n        self.metrics['requests'].append((timestamp, model_id, tokens, latency, cost))\n        \n        # Send to CloudWatch\n        self.cloudwatch.put_metric_data(\n            Namespace='Bedrock/Application',\n            MetricData=[\n                {\n                    'MetricName': 'RequestLatency',\n                    'Value': latency,\n                    'Unit': 'Seconds',\n                    'Dimensions': [{'Name': 'ModelId', 'Value': model_id}]\n                },\n                {\n                    'MetricName': 'TokensProcessed',\n                    'Value': tokens,\n                    'Unit': 'Count'\n                },\n                {\n                    'MetricName': 'RequestCost',\n                    'Value': cost,\n                    'Unit': 'None'\n                }\n            ]\n        )\n```\n\nüíé **Key optimization principles**:\n- Monitor and measure everything\n- Cache aggressively for repeated queries\n- Use appropriate models for complexity levels\n- Implement circuit breakers for reliability\n- Continuously evaluate and improve quality"
      }
    ]
  },
  "5": {
    "sections": [
      {
        "title": "Security and Compliance",
        "content": "### Security and Compliance\n\nProduction AI systems require robust security frameworks to protect sensitive data and maintain compliance standards. Implementing comprehensive security measures is essential for enterprise-grade Bedrock applications.\n\n**IAM Policy Implementation**\n\nImplement least-privilege access principles with granular IAM policies:\n\n```json\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"bedrock:InvokeModel\"\n      ],\n      \"Resource\": \"arn:aws:bedrock:*:*:foundation-model/anthropic.claude-v2\",\n      \"Condition\": {\n        \"StringEquals\": {\n          \"aws:RequestedRegion\": \"us-east-1\"\n        }\n      }\n    }\n  ]\n}\n```\n\nüîß **Best Practice**: Create service-specific roles rather than using broad permissions.\n\n**Encryption and Data Protection**\n\nImplement encryption at rest and in transit:\n\n- **KMS Integration**: Use AWS KMS for encrypting application data and logs\n- **TLS Configuration**: Enforce HTTPS/TLS 1.2+ for all API communications\n- **Data Masking**: Implement PII detection and masking before model inference\n\n**VPC Endpoints and Network Security**\n\nSecure network architecture using VPC endpoints:\n\n```python\n# Configure VPC endpoint for Bedrock\nendpoint_config = {\n    'VpcId': 'vpc-12345678',\n    'ServiceName': 'com.amazonaws.us-east-1.bedrock-runtime',\n    'SubnetIds': ['subnet-12345678'],\n    'SecurityGroupIds': ['sg-bedrock-access']\n}\n```\n\n**Compliance Requirements**\n\n‚úÖ **Key Compliance Areas**:\n- **GDPR**: Implement data subject rights and consent management\n- **HIPAA**: Ensure BAAs are in place and audit logging is enabled\n- **SOC 2**: Maintain access controls and incident response procedures\n- **Data Residency**: Configure region-specific deployments\n\nüí° **Pro Tip**: Use AWS Config rules to continuously monitor compliance posture and automatically remediate policy violations.\n\n‚ö†Ô∏è **Security Reminder**: Regularly rotate API keys, review access logs, and conduct security assessments to maintain robust protection against evolving threats."
      },
      {
        "title": "Monitoring and Observability",
        "content": "### Monitoring and Observability\n\nComprehensive monitoring ensures optimal performance and quick issue resolution in production Bedrock applications. Implementing robust observability practices enables proactive system management.\n\n**CloudWatch Metrics and Alarms**\n\nSet up custom metrics for Bedrock applications:\n\n```python\nimport boto3\nfrom datetime import datetime\n\ncloudwatch = boto3.client('cloudwatch')\n\n# Custom metric for model response time\ndef put_custom_metric(response_time, model_id):\n    cloudwatch.put_metric_data(\n        Namespace='Bedrock/Application',\n        MetricData=[\n            {\n                'MetricName': 'ResponseTime',\n                'Dimensions': [\n                    {'Name': 'ModelId', 'Value': model_id}\n                ],\n                'Value': response_time,\n                'Unit': 'Milliseconds',\n                'Timestamp': datetime.utcnow()\n            }\n        ]\n    )\n```\n\nüéì **Key Metrics to Monitor**:\n- Request latency and throughput\n- Token usage and costs\n- Error rates and types\n- Model availability and performance\n\n**Logging Strategies**\n\nImplement structured logging for better observability:\n\n```python\nimport logging\nimport json\n\n# Structured logging setup\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef log_bedrock_request(request_id, model_id, tokens_used, response_time):\n    log_data = {\n        'timestamp': datetime.utcnow().isoformat(),\n        'request_id': request_id,\n        'model_id': model_id,\n        'tokens_used': tokens_used,\n        'response_time_ms': response_time,\n        'service': 'bedrock-api'\n    }\n    logger.info(json.dumps(log_data))\n```\n\n**Performance Monitoring Dashboard**\n\nüíé **Dashboard Components**:\n- Real-time request volume and success rates\n- Model performance comparisons\n- Cost tracking and budget alerts\n- User experience metrics (latency percentiles)\n\n**Alerting Configuration**\n\n‚ö†Ô∏è **Critical Alerts**:\n- Error rate > 5%\n- Average latency > 10 seconds\n- Daily cost exceeds budget threshold\n- Model throttling incidents\n\nüîß **Monitoring Best Practice**: Implement distributed tracing using AWS X-Ray to track requests across services and identify bottlenecks in complex AI workflows."
      },
      {
        "title": "Scalability and Architecture Patterns",
        "content": "### Scalability and Architecture Patterns\n\nDesigning scalable architectures ensures your Bedrock applications can handle varying loads while maintaining performance and cost-effectiveness. Strategic architectural decisions are crucial for production success.\n\n**Load Balancing Strategies**\n\nImplement intelligent request distribution:\n\n```python\nimport random\nfrom typing import List, Dict\n\nclass ModelLoadBalancer:\n    def __init__(self, models: List[Dict]):\n        self.models = models\n        self.request_counts = {model['id']: 0 for model in models}\n    \n    def select_model(self, request_type: str):\n        # Weighted round-robin based on capacity\n        available_models = [m for m in self.models \n                          if m['status'] == 'active' and \n                          self.request_counts[m['id']] < m['max_concurrent']]\n        \n        if not available_models:\n            raise Exception(\"No available models\")\n        \n        # Select model with lowest current load\n        selected = min(available_models, \n                      key=lambda m: self.request_counts[m['id']])\n        self.request_counts[selected['id']] += 1\n        return selected['id']\n```\n\n**Caching Implementation**\n\nüîß **Multi-Layer Caching Strategy**:\n\n```python\nimport redis\nimport hashlib\nimport json\n\nclass BedrockCache:\n    def __init__(self):\n        self.redis_client = redis.Redis(host='elasticache-endpoint')\n        self.local_cache = {}\n    \n    def get_cached_response(self, prompt: str, model_id: str):\n        cache_key = hashlib.md5(f\"{prompt}:{model_id}\".encode()).hexdigest()\n        \n        # Check local cache first (fastest)\n        if cache_key in self.local_cache:\n            return self.local_cache[cache_key]\n        \n        # Check Redis cache (shared across instances)\n        cached = self.redis_client.get(cache_key)\n        if cached:\n            response = json.loads(cached)\n            self.local_cache[cache_key] = response  # Populate local cache\n            return response\n        \n        return None\n```\n\n**Architecture Patterns**\n\nüíé **Recommended Patterns**:\n- **API Gateway + Lambda**: Serverless scaling with built-in throttling\n- **ECS with Auto Scaling**: Container-based with predictable scaling\n- **Event-Driven Processing**: SQS/SNS for asynchronous AI workflows\n- **Multi-Region Deployment**: Geographic distribution for low latency\n\n**Resource Optimization**\n\n‚úÖ **Efficiency Strategies**:\n- Implement connection pooling for Bedrock clients\n- Use batch processing for multiple similar requests\n- Configure appropriate timeout and retry policies\n- Implement circuit breaker patterns for fault tolerance\n\nüéì **Scaling Best Practice**: Monitor queue depths and response times to trigger auto-scaling events, ensuring optimal resource utilization while maintaining user experience quality."
      },
      {
        "title": "Testing and Quality Assurance",
        "content": "### Testing and Quality Assurance\n\nRobust testing strategies ensure AI applications maintain quality and reliability in production. Comprehensive QA processes are essential for managing the unique challenges of AI system testing.\n\n**Model Evaluation Framework**\n\nImplement systematic model testing:\n\n```python\nimport pandas as pd\nfrom sklearn.metrics import accuracy_score\nimport boto3\n\nclass BedrockModelTester:\n    def __init__(self, model_id: str):\n        self.bedrock = boto3.client('bedrock-runtime')\n        self.model_id = model_id\n    \n    def run_evaluation_suite(self, test_dataset: pd.DataFrame):\n        results = []\n        \n        for _, row in test_dataset.iterrows():\n            try:\n                response = self.bedrock.invoke_model(\n                    modelId=self.model_id,\n                    body=json.dumps({\n                        'prompt': row['prompt'],\n                        'max_tokens': 100\n                    })\n                )\n                \n                prediction = json.loads(response['body'].read())['completion']\n                \n                results.append({\n                    'test_id': row['id'],\n                    'expected': row['expected_output'],\n                    'actual': prediction,\n                    'passed': self.evaluate_response(prediction, row['expected_output'])\n                })\n            except Exception as e:\n                results.append({\n                    'test_id': row['id'],\n                    'error': str(e),\n                    'passed': False\n                })\n        \n        return pd.DataFrame(results)\n```\n\n**Regression Testing Strategy**\n\nüéì **Automated Test Categories**:\n- **Functionality Tests**: Core feature validation\n- **Performance Tests**: Latency and throughput benchmarks\n- **Security Tests**: Input validation and injection prevention\n- **Integration Tests**: End-to-end workflow validation\n\n**A/B Testing Implementation**\n\n```python\nimport random\n\nclass ABTestManager:\n    def __init__(self, control_model: str, test_model: str, split_ratio: float = 0.5):\n        self.control_model = control_model\n        self.test_model = test_model\n        self.split_ratio = split_ratio\n    \n    def assign_variant(self, user_id: str):\n        # Consistent assignment based on user ID hash\n        hash_value = hash(user_id) % 100\n        return self.test_model if hash_value < (self.split_ratio * 100) else self.control_model\n```\n\n**Quality Metrics Tracking**\n\nüí° **Key QA Metrics**:\n- Response relevance scores\n- Hallucination detection rates\n- Safety and bias assessments\n- User satisfaction feedback\n\n**Continuous Testing Pipeline**\n\n‚úÖ **CI/CD Integration**:\n\n```yaml\n# GitHub Actions example\nname: AI Model Testing\non: [push, pull_request]\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v2\n      - name: Run Model Tests\n        run: |\n          python -m pytest tests/model_tests.py\n          python scripts/evaluate_model_performance.py\n```\n\n‚ö†Ô∏è **Testing Reminder**: AI models can be non-deterministic, so implement statistical significance testing and multiple evaluation runs to ensure reliable quality assessments."
      },
      {
        "title": "Cost Management and Optimization",
        "content": "### Cost Management and Optimization\n\nEffective cost management is crucial for sustainable AI operations. Implementing strategic optimization techniques helps maintain budget control while maximizing value from Bedrock services.\n\n**Usage Pattern Analysis**\n\nMonitor and analyze cost drivers:\n\n```python\nimport boto3\nimport pandas as pd\nfrom datetime import datetime, timedelta\n\nclass CostAnalyzer:\n    def __init__(self):\n        self.ce_client = boto3.client('ce')\n    \n    def get_bedrock_costs(self, days: int = 30):\n        end_date = datetime.now()\n        start_date = end_date - timedelta(days=days)\n        \n        response = self.ce_client.get_cost_and_usage(\n            TimePeriod={\n                'Start': start_date.strftime('%Y-%m-%d'),\n                'End': end_date.strftime('%Y-%m-%d')\n            },\n            Granularity='DAILY',\n            Metrics=['BlendedCost', 'UsageQuantity'],\n            GroupBy=[\n                {'Type': 'DIMENSION', 'Key': 'SERVICE'},\n                {'Type': 'DIMENSION', 'Key': 'USAGE_TYPE'}\n            ],\n            Filter={\n                'Dimensions': {\n                    'Key': 'SERVICE',\n                    'Values': ['Amazon Bedrock']\n                }\n            }\n        )\n        \n        return self.parse_cost_data(response)\n```\n\n**Budget Controls and Alerts**\n\nüîß **Automated Budget Management**:\n\n```python\ndef setup_budget_alerts(budget_amount: float, alert_thresholds: list):\n    budgets_client = boto3.client('budgets')\n    \n    budget = {\n        'BudgetName': 'Bedrock-AI-Budget',\n        'BudgetLimit': {\n            'Amount': str(budget_amount),\n            'Unit': 'USD'\n        },\n        'TimeUnit': 'MONTHLY',\n        'BudgetType': 'COST',\n        'CostFilters': {\n            'Service': ['Amazon Bedrock']\n        }\n    }\n    \n    subscribers = [{\n        'SubscriptionType': 'EMAIL',\n        'Address': 'admin@company.com'\n    }]\n    \n    for threshold in alert_thresholds:\n        notification = {\n            'Notification': {\n                'NotificationType': 'ACTUAL',\n                'ComparisonOperator': 'GREATER_THAN',\n                'Threshold': threshold,\n                'ThresholdType': 'PERCENTAGE'\n            },\n            'Subscribers': subscribers\n        }\n```\n\n**Optimization Strategies**\n\nüíé **Cost Reduction Techniques**:\n\n- **Model Selection**: Use smaller models for simple tasks\n- **Prompt Engineering**: Optimize prompts to reduce token usage\n- **Caching**: Implement intelligent caching to avoid duplicate requests\n- **Request Batching**: Group similar requests for efficiency\n\n**Token Usage Optimization**\n\n```python\nclass TokenOptimizer:\n    def __init__(self):\n        self.prompt_cache = {}\n    \n    def optimize_prompt(self, user_input: str, context: str):\n        # Remove redundant context\n        optimized_context = self.compress_context(context)\n        \n        # Use efficient prompt structure\n        optimized_prompt = f\"Context: {optimized_context}\\n\\nUser: {user_input}\\n\\nAssistant:\"\n        \n        return optimized_prompt\n    \n    def compress_context(self, context: str, max_tokens: int = 1000):\n        # Implement context compression logic\n        if len(context.split()) <= max_tokens:\n            return context\n        \n        # Keep most relevant parts\n        sentences = context.split('.')\n        return '. '.join(sentences[:max_tokens//20])\n```\n\nüéì **Best Practice**: Implement tiered pricing strategies where simple queries use cost-effective models, while complex tasks utilize premium models only when necessary.\n\n‚úÖ **Cost Monitoring Checklist**:\n- Set up daily cost alerts\n- Track token usage per feature\n- Monitor model performance vs. cost ratios\n- Regular review of usage patterns and optimization opportunities"
      }
    ]
  }
};
